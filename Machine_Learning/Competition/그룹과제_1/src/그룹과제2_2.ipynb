{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "plt.style.use(['fivethirtyeight'])\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#EDA\n",
    "\n",
    "# from pandas_profiling import ProfileReport\n",
    "#profile = ProfileReport(df, title='Pandas Profiling Report')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "\n",
    "import klib\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Modeling\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/usr/local/share/fonts\"\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Ubuntu':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)\n",
    "else:\n",
    "    print('Unknown system...')\n",
    "rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 **EDA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.abspath(\"../dat\")+'/train_transactions.csv', encoding='cp949')\n",
    "\n",
    "test = pd.read_csv(os.path.abspath(\"../dat\")+'/test_transactions.csv', encoding='cp949')\n",
    "\n",
    "y_train = pd.read_csv(os.path.abspath(\"../dat\")+'/y_train.csv').gender\n",
    "\n",
    "train_id = train['cust_id']\n",
    "test_id = test['cust_id']\n",
    "\n",
    "\n",
    "tr = pd.concat([train, test]).reset_index(drop=True)\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  **Feature Enginering**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame({'cust_id': tr.cust_id.unique()})\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 'gds_grp_nm'\n",
    "\n",
    "features = pd.pivot_table(tr, index='cust_id', columns=level, values='amount',\n",
    "                            aggfunc=lambda x: len(x), fill_value=0).reset_index()\n",
    "# #train_test = pd.pivot_table(pd.concat([df_train, df_test]), index='cust_id', columns=level, values='amount',\n",
    "# #                            aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0).reset_index()\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.groupby('cust_id')['amount'].agg([\n",
    "    ('총구매액',np.sum), \n",
    "    ('구매건수', np.size), \n",
    "    ('평균구매액', lambda x: np.round(np.mean(x))),\n",
    "    ('최대구매액', np.max),\n",
    "]).reset_index()\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.to_datetime(tr.tran_date, format=\"%Y-%m-%d %H:%M\").dt.month\n",
    "x.groupby(features['cust_id'])\n",
    "features['월'] = x\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.to_datetime(tr.tran_date, format=\"%Y-%m-%d %H:%M\").dt.day\n",
    "x.groupby(features['cust_id'])\n",
    "features['일'] = x\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = tr.groupby('cust_id')['gds_grp_mclas_nm'].agg([\n",
    "#     (('최대제품', np.max))\n",
    "# ]).reset_index()\n",
    "# features = features.merge(f, how='left', left_index=False); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  휴면 여부\n",
    "f = tr.groupby('cust_id')['tran_date'].agg([\n",
    "     ('휴면_여부', lambda x: int((tr.tran_date.astype('datetime64').max() - x.astype('datetime64').max()).days))\n",
    " ]).reset_index()\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.query('gds_grp_mclas_nm == \"화장품\"').groupby('cust_id')['amount'].agg([\n",
    "    ('화장품 구매금액', np.sum),\n",
    "    ('화장품 구매건수', np.size)\n",
    "]).reset_index()\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.groupby('cust_id')['tran_date'].agg([\n",
    "       ('평일방문비율', lambda x: np.mean(pd.to_datetime(x).dt.dayofweek>4))\n",
    "    ]).reset_index()\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.groupby('cust_id')['tran_date'].agg([\n",
    "    ('내점일수',lambda x: x.str[:10].nunique()),\n",
    "    ('구매주기', lambda x: int((x.astype('datetime64').max() - x.astype('datetime64').min()).days / x.str[:10].nunique())),\n",
    "    ('봄-구매비율', lambda x: np.mean(pd.to_datetime(x).dt.month.isin([3,4,5]))),\n",
    "    ('여름-구매비율', lambda x: np.mean(pd.to_datetime(x).dt.month.isin([6,7,8]))),\n",
    "    ('가을-구매비율', lambda x: np.mean(pd.to_datetime(x).dt.month.isin([9,10,11]))),\n",
    "    ('겨울-구매비율', lambda x: np.mean(pd.to_datetime(x).dt.month.isin([1,2,12])))\n",
    "]).reset_index()\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr[tr.amount < 0].groupby('cust_id')['amount'].agg([\n",
    "    ('환불금액', lambda x: x.sum() * -1),\n",
    "    ('환불건수', np.size)\n",
    "]).reset_index()\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = tr.groupby('cust_id')['tran_date'].agg(lambda x: x.nunique())\n",
    "f = (tr.groupby('cust_id')['amount'].sum() / visits).reset_index().rename(columns={0 : \"내점당구매액\"})\n",
    "features = features.merge(f, how='left'); features\n",
    "f = (tr.groupby('cust_id')['amount'].size() / visits).reset_index().rename(columns={0 : \"내점당구매건수\"})\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.groupby('cust_id')['gds_grp_mclas_nm'].agg([\n",
    "    ('주구매상품', lambda x: x.value_counts().index[0])\n",
    "]).reset_index()\n",
    "features = features.merge(f, how='left'); features['주구매상품'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.groupby('cust_id')['store_nm'].agg([\n",
    "    ('주구매지점', lambda x: x.value_counts().index[0])\n",
    "]).reset_index()\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['tran_month'] = tr['tran_date'].apply(lambda x: x[5:7])\n",
    "\n",
    "month = tr.groupby([tr['cust_id'],tr['tran_month']])['amount'].count().reset_index()\n",
    "month.tran_month = month.tran_month.apply(lambda x: int(x))\n",
    "df = pd.DataFrame(columns = ['cust_id', 'tran_month'])\n",
    "for cust_id in range(0,5982):\n",
    "    for tran_month in range(1,13):\n",
    "        df = df.append(pd.DataFrame([[cust_id, tran_month]], columns = ['cust_id', 'tran_month']))\n",
    "\n",
    "df = df.merge(month, how='left')\n",
    "df = df.fillna(0)\n",
    "\n",
    "df1 = pd.DataFrame(columns = ['cust_id', '기울기'])\n",
    "for i in range(0,df.cust_id.nunique()):\n",
    "    a = df.query('cust_id == @i').tran_month\n",
    "    b = df.query('cust_id == @i').amount\n",
    "    c = np.polyfit(a, b, 1)[0].round(2)\n",
    "    df1 = df1.append(pd.DataFrame([[i, c]], columns = ['cust_id', '구매 추세 기울기']))\n",
    "\n",
    "features = features.merge(df1, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.drop(columns= ['기울기'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.query('gds_grp_mclas_nm == \"화장품\"').groupby('cust_id')['tran_date'].agg([('화장품구매주기', lambda x: int((x.astype('datetime64').max() - x.astype('datetime64').min()).days / x.str[:10].nunique()))]).reset_index()\n",
    "features = features.merge(f, how='left');features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = ['축산가공', '육류', '수산품', '젓갈/반찬', '가공식품', '농산물', '차/커피', '주류']\n",
    "f = tr.groupby('cust_id')['amount'].agg([('총구매액', np.sum)]).reset_index()\n",
    "food_amount = tr.query('gds_grp_mclas_nm == @food').groupby('cust_id')['amount'].agg([\n",
    "    ('식료품구매액', np.sum)\n",
    "]).reset_index()\n",
    "f = pd.merge(f, food_amount, on = 'cust_id', how = 'left').fillna(0)\n",
    "f['식료품구매액비율'] = f['식료품구매액'] / f['총구매액']\n",
    "features = features.merge(f[['cust_id', '식료품구매액비율']], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = tr.groupby('goods_id').mean().reset_index().drop(['cust_id'], axis = 1)\n",
    "\n",
    "price.columns = ['goods_id', 'price']\n",
    "\n",
    "price_4q = price['price'].quantile([.25,.5,.75])\n",
    "\n",
    "tr = tr.merge(price, how = 'left')\n",
    "\n",
    "f = tr.groupby('cust_id')['price'].agg([\n",
    "    ('고가상품구매율', lambda x : (x > price_4q.iloc[2]).mean().round(2))\n",
    "]).reset_index()\n",
    "\n",
    "features = features.merge(f, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sellers = ['농산물', '가공식품', '축산가공', '수산품', '화장품', '디자이너', '시티웨어', '일용잡화', '육류', '차/커피'] # 베스트 셀러 대분류\n",
    "\n",
    "f = tr.groupby('cust_id')['amount'].agg([('구매건수', np.size)]).reset_index()\n",
    "\n",
    "b_s = tr.query('gds_grp_mclas_nm == @best_sellers').groupby('cust_id')['amount'].agg([\n",
    "    ('베스트셀러구매건수', np.size)\n",
    "]).reset_index()\n",
    "\n",
    "f = pd.merge(f, b_s, on = 'cust_id', how = 'left').fillna(0)\n",
    "\n",
    "f['베스트셀러구매비율'] = f['베스트셀러구매건수'] / f['구매건수']\n",
    "features = features.merge(f[['cust_id', '베스트셀러구매비율']], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.groupby('cust_id')['amount'].agg([('구매건수', np.size)]).reset_index()\n",
    "\n",
    "g = tr[tr.amount < 0].groupby('cust_id')['amount'].agg([('환불건수', np.size)]).reset_index()\n",
    "\n",
    "f = f.merge(g, how = 'left').fillna(0)\n",
    "f['환불비율'] = f['환불건수'] / f['구매건수']\n",
    "\n",
    "features = features.merge(f[['cust_id', '환불비율']], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.query(\"gds_grp_nm == '스타킹(특정)'\").groupby('cust_id')['amount'].agg([\n",
    "    ('스타킹구매건수', np.size)\n",
    "]).reset_index()\n",
    "\n",
    "features = features.merge(f, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = tr.groupby('cust_id')['tran_date'].agg([\n",
    "    ('구매주기', lambda x: int((x.astype('datetime64').max() - x.astype('datetime64').min()).days / x.str[:10].nunique()))]).reset_index()\n",
    "f = tr.groupby('cust_id')['amount'].agg([ \n",
    "    ('구매건수', np.size)]).reset_index()\n",
    "m = tr.groupby('cust_id')['amount'].agg([ \n",
    "    ('총구매액', np.sum)]).reset_index()\n",
    "rfm = r.merge(f).merge(m)\n",
    "r_labels = range(4, 0, -1) \n",
    "f_labels = range(1, 5)\n",
    "m_labels = range(1, 5)\n",
    "\n",
    "rfm['r'] = pd.qcut(rfm['구매주기'], q=4, labels=r_labels).astype(int)\n",
    "rfm['f'] = pd.qcut(rfm['구매건수'], q=4, labels=f_labels).astype(int)\n",
    "rfm['m'] = pd.qcut(rfm['총구매액'], q=4, labels=m_labels).astype(int)\n",
    "rfm_s = [2, 2, 3]\n",
    "rfm['rfm'] = rfm_s[0] * rfm['r'] + rfm_s[1] * rfm['f'] + rfm_s[2] * rfm['m']\n",
    "seg_map = { \n",
    "    r'[1-2][1-2]': '수면고객',\n",
    "    r'13': '이탈위험고객',\n",
    "    r'14': '관심대상고객',\n",
    "    r'23': '3등급고객',\n",
    "    r'24': '2.5등급고객',\n",
    "    r'[3-4]1': '신규고객',\n",
    "    r'32': '3등급고객',\n",
    "    r'42': '2.5등급고객',\n",
    "    r'33': '2등급고객',\n",
    "    r'43': '1등급고객',\n",
    "    r'34': '1등급고객',\n",
    "    r'44': 'vip고객'\n",
    "}\n",
    "\n",
    "rfm['고객등급'] = rfm['r'].map(str) + rfm['f'].map(str)\n",
    "rfm['고객등급'] = rfm['고객등급'].replace(seg_map, regex=True)\n",
    "rfm = rfm.drop(\"구매주기\", axis=1)\n",
    "rfm = rfm.drop(\"구매건수\", axis=1)\n",
    "rfm = rfm.drop(\"총구매액\", axis=1)\n",
    "rfm = rfm.drop(\"r\", axis=1)\n",
    "rfm = rfm.drop(\"f\", axis=1)\n",
    "rfm = rfm.drop(\"m\", axis=1)\n",
    "rfm = rfm.drop(\"rfm\", axis=1)\n",
    "features = features.merge(rfm, how='left');features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tr.groupby('cust_id')['amount'].sum() / tr.amount.mean()\n",
    "t = t.astype(int).reset_index();t\n",
    "t = t.rename(columns = {'amount' : '평균대비구매비중'});t\n",
    "features = features.merge(t, how='left');features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = tr[tr.amount < 0].groupby('cust_id')['gds_grp_nm'].agg([\n",
    "#     ('주환불상품', lambda x: x.value_counts().index[0])\n",
    "# ]).reset_index();t\n",
    "# features = features.merge(t, how='left');features['주환불상품'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['남성'] = tr['gds_grp_mclas_nm'].str.contains('남성').astype(int)\n",
    "t = tr[tr.남성 ==1].groupby('cust_id')['amount'].agg([\n",
    "    (\"남성포함상품구매건수\" , np.size)\n",
    "]).reset_index()\n",
    "features = features.merge(t, how='left');features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['여성'] = tr['gds_grp_mclas_nm'].str.startswith(('여','화장품','주방','란제리')).astype(int)\n",
    "t = tr[tr.여성 ==1].groupby('cust_id')['amount'].agg([\n",
    "    (\"여성키워드상품구매건수\" , np.size)\n",
    "]).reset_index()\n",
    "features = features.merge(t, how='left');features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.groupby('cust_id')['store_nm'].agg([('다양한 매장 방문','nunique')]).reset_index() # 다양한 매장 방문\n",
    "features = features.merge(f, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade(x):\n",
    "    if x <= 500000:\n",
    "        return 1\n",
    "    elif x <= 2000000:\n",
    "        return 2\n",
    "    elif x <= 5000000:\n",
    "        return 3\n",
    "    elif x <= 10000000:\n",
    "        return 4\n",
    "    elif x <= 50000000:\n",
    "        return 5\n",
    "\n",
    "    \n",
    "f = tr.groupby('cust_id')['amount'].agg([\n",
    "    ('가격선호도', lambda x: grade(np.mean(x)))\n",
    "]).reset_index()\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifeStyle(x):\n",
    "    if x in ['농산물', '가공식품', '축산가공', '수산품', '육류', '차/커피', '젓갈/반찬', '건강식품', '주류']:\n",
    "        return '식품'\n",
    "    elif x in ['주방가전', '대형가전', '가구', '소형가전', '통신/컴퓨터', '전자/전기']:\n",
    "        return '가전'\n",
    "    elif x in ['일용잡화', '주방용품', '생활잡화', '식기', '침구/수예', '문구/팬시']:\n",
    "        return '생활용품'\n",
    "    elif x in ['골프', '스포츠', '악기']:\n",
    "        return '예체능'\n",
    "    elif x in ['액세서리', '준보석/시계', '디자이너', '명품', '보석']:\n",
    "        return '명품'\n",
    "    elif x in ['트래디셔널', '시티웨어', '섬유잡화', '캐주얼', '구두', '기타의류', '교복', '커리어']:\n",
    "        return '의류'\n",
    "    elif x in ['남성 캐주얼', '셔츠', '남성정장', '남성 트랜디']:\n",
    "        return '남성의류'\n",
    "    elif x in ['모피/피혁', '피혁잡화', '웨딩', '멀티(아울렛)', '란제리/내의']:\n",
    "        return '여성의류'\n",
    "    elif x in ['아동']:\n",
    "        return '아동용품'\n",
    "    elif x in ['화장품']:\n",
    "        return '화장품'\n",
    "    elif x in ['기타']:\n",
    "        return '기타'\n",
    "    \n",
    "tr[\"lifeStyle\"]=tr['gds_grp_mclas_nm'].apply(lifeStyle)\n",
    "\n",
    "f = pd.pivot_table(tr, index='cust_id', columns='lifeStyle', values='amount', aggfunc=np.mean, fill_value=0).reset_index()\n",
    "a = f.iloc[:,1:12] \n",
    "a['라이프 스타일(평균)'] = a.idxmax(axis=1)\n",
    "f = pd.concat([f, a], axis=1)['라이프 스타일(평균)'].reset_index(name = '라이프 스타일(평균)').rename(columns = {'index' : 'cust_id'})\n",
    "features = features.merge(f, how='left'); features\n",
    "\n",
    "f = pd.pivot_table(tr, index='cust_id', columns='lifeStyle', values='amount', aggfunc=np.size, fill_value=0).reset_index()\n",
    "a = f.iloc[:,1:12] \n",
    "a['라이프 스타일(건수)'] = a.idxmax(axis=1)\n",
    "f = pd.concat([f, a], axis=1)['라이프 스타일(건수)'].reset_index(name = '라이프 스타일(건수)').rename(columns = {'index' : 'cust_id'})\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['요일'] = tr['tran_date'].agg(lambda x: ('월', '화', '수', '목', '금', '토', '일')[pd.to_datetime(x).weekday()])\n",
    "\n",
    "f = tr.groupby('cust_id')['요일'].agg([('주구매 요일', lambda x: x.value_counts().index[0])]).reset_index()\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['month'] = tr['tran_date'].agg(lambda x: pd.to_datetime(x).month)\n",
    "tr['season'] = tr['month'].agg(lambda x: 'spring' if x in [3,4,5] else 'summer' if x in [6,7,8]\n",
    "                              else 'fall' if x in [9,10,11] else 'fall')\n",
    "f = tr.groupby('cust_id')['season'].agg([\n",
    "    ('선호방문계절', lambda x: x.value_counts().index[0])\n",
    "])\n",
    "f['선호방문월'] = tr.groupby('cust_id')['month'].apply(lambda x: x.value_counts().index[0])\n",
    "f\n",
    "features = features.merge(f, on = 'cust_id', how = 'left') ; features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr[tr.gds_grp_mclas_nm.str.contains('행사') | tr.gds_grp_nm.str.contains('행사')]\n",
    "f = f.groupby('cust_id')['amount'].agg([\n",
    "    ('행사상품구매수','count')\n",
    "])\n",
    "features = features.merge(f, on = 'cust_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_4q = tr.groupby('goods_id')['amount'].mean().quantile([.25,.5,.75])\n",
    "price_4q\n",
    "f = (tr.groupby('cust_id')['amount']\n",
    "     .agg([('저가상품구매율', lambda x: (x < price_4q.iloc[0]).mean().round(2))])\n",
    "     .reset_index())\n",
    "f\n",
    "features = features.merge(f, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.tran_date = tr.tran_date.astype(str).astype('datetime64') \n",
    "#요일 컬럼 추가\n",
    "tr['weekday_name'] = tr['tran_date'].dt.day_name()\n",
    "tr['weekday'] = tr['tran_date'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.pivot_table(tr, index = 'cust_id', columns = 'weekday', values = 'amount', aggfunc = ['count'])\n",
    "f1 = f.transpose()/f.transpose().sum()\n",
    "f = f1.transpose()\n",
    "f = f.fillna(0)\n",
    "f2 = np.apply_along_axis(lambda x: np.max(x) - np.min(x), 1, f)\n",
    "features = pd.concat((features, pd.DataFrame(f2)), axis = 1)\n",
    "features = features.rename({0:'요일 간 구매건수 편차'}, axis = 'columns');features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.groupby('cust_id')['amount'].agg([\n",
    "    ('총구매액',np.sum)\n",
    "]).reset_index()\n",
    "features = features.merge(f, how='left');\n",
    "visits = tr.groupby('cust_id')['tran_date'].agg(lambda x: x.nunique())\n",
    "f = (tr.groupby('cust_id')['amount'].sum() / visits).reset_index().rename(columns={0 : \"내점당구매액\"})\n",
    "features = features.merge(f, how='left');\n",
    "features[\"충동지수\"] = features[\"내점당구매액\"]/features[\"총구매액\"]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_goods = ['N.B정장','N/B골프의류','주류','직수입 골프의류','스포츠화',\n",
    "             'L/C골프의류','L/C정장','단기행사(골프)','스포츠의류','레포츠',\n",
    "             '안주류','시계','넥타이(특정)','고정행사(골프)','등산화','골프용품',\n",
    "             '명품남성','셔츠고정행사','셔츠단기행사','명품시계기타',\n",
    "             '기타(가발,포장,담배,끽연구,사진,수선)','정상스포츠','단기행사(레져)',\n",
    "             '고정행사(레져)','시티정장바지','넥타이핀','직수입정장','맞춤셔츠',\n",
    "             '셔츠 직매입(PB)','IT 게임기','명품시계직매입','남성잡화멀티',\n",
    "             '정장점행사','NB 남화','남성시티 직매입']\n",
    "\n",
    "woman_goods = ['디자이너부틱','어덜트','란제리행사','뷰티상품','스타킹(특정)','보석',\n",
    "               '크리스탈','란제리','디자이너','색조 화장품','NB 여화','일반핸드백',\n",
    "               '직수입핸드백','숙녀단기행사','숙녀고정행사','영플라자(핸드백)','14K',\n",
    "               '모피','머플러(특정)','명품잡화보석','명품보석','수입잡화보석',\n",
    "               '주얼리멀티','란제리(멀티)','스카프(특정)']\n",
    "tr['man_goods'] = tr['gds_grp_nm'].apply(lambda x: 1 if x in man_goods else 0)\n",
    "tr['woman_goods'] = tr['gds_grp_nm'].apply(lambda x: 1 if x in woman_goods else 0)     # 상품군에 포함되는 경우 1을 주었다.\n",
    "f = tr.groupby('cust_id')['man_goods'].agg([('남성용품구매건수',np.sum)]).reset_index()      # 상품들을 더해 횟수를 구했다.\n",
    "features = features.merge(f, how='left')\n",
    "f = tr.groupby('cust_id')['woman_goods'].agg([('여성용품구매건수',np.sum)]).reset_index()\n",
    "features = features.merge(f, how='left')\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tr.query('amount < 0')   \n",
    "a = d.groupby('goods_id')['amount'].count().quantile(.99) \n",
    "z = d.groupby('goods_id')['amount'].count().reset_index() \n",
    "refund = list(z.query('amount >= @a')['goods_id'].unique()) \n",
    "d = tr.groupby([tr['cust_id'],tr['goods_id'],tr['gds_grp_nm']])['amount'].count().reset_index()\n",
    "\n",
    "no_repay = list(d.query('amount < 2')['goods_id'].unique())\n",
    "\n",
    "no_s = list(set(refund).intersection(no_repay)) \n",
    "tr['no_s'] = tr['goods_id'].apply(lambda x: 1 if x in no_s else 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.query('no_s == 1').groupby('cust_id')['amount'].agg([('만족도 떨어지는 제품 총 구입금액',np.sum)]).reset_index()\n",
    "features = features.merge(f, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.groupby('cust_id')['no_s'].agg([('만족도 떨어지는 제품 총 구매건수',np.sum)]).reset_index()\n",
    "features = features.merge(f, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "time = dt.datetime(2007,12,31)\n",
    "time\n",
    "f = tr.groupby('cust_id')['tran_date'].agg([('휴면일수', lambda x : (time - x.astype('datetime64').max()).days)]).reset_index(); f\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr[tr.amount > 0].groupby('cust_id')['gds_grp_mclas_nm'].agg([('취미용품 구매비율', lambda x: (list(x).count('골프') + list(x).count('스포츠') + list(x).count('통신/컴퓨터'))/len(x))]).reset_index(); f\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr[tr.amount > 0].groupby('cust_id')['gds_grp_mclas_nm'].agg([('아동용품 구매건수', lambda x: list(x).count('아동')+list(x).count('구두'))]).reset_index(); f\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr_holidays = ['2007-02-13 00:00:00', '2007-02-14 00:00:00', '2007-02-15 00:00:00', '2007-02-16 00:00:00', '2007-02-17 00:00:00', \n",
    "               '2007-02-18 00:00:00', '2007-02-19 00:00:00',\n",
    "               '2007-09-20 00:00:00', '2007-09-21 00:00:00', '2007-09-22 00:00:00', '2007-09-23 00:00:00', '2007-09-24 00:00:00', \n",
    "               '2007-09-25 00:00:00', '2007-09-26 00:00:00']\n",
    "holi_nm=['건강식품', '육류', '수산품', '축산가공', '젓갈/반찬', '가공식품', '농산물', '주류']\n",
    "f = tr.query(\"tran_date == @kr_holidays\").query(\"gds_grp_mclas_nm == @holi_nm\").groupby('cust_id')['amount'].agg([\n",
    "    ('명절식품비용', np.sum)]).reset_index()\n",
    "\n",
    "t = tr.groupby('cust_id')['amount'].agg([('총구매액',np.sum)])\n",
    "f = pd.merge(t,f,on = 'cust_id', how ='left').fillna(0)\n",
    "f\n",
    "f = f[['cust_id','명절식품비용']]\n",
    "features = features.merge(f,on='cust_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# munchies = ['양과(세트행사)', 'takeout양과', '즉석.스넥(매장)', '브랜드샵',\n",
    "#             '수입단기행사', '브랑제리', '스낵형 델리', '가공선물세트', '제과제빵', '캔디', '케익류']\n",
    "\n",
    "# valentine = ['2007-02-12 00:00:00', '2007-02-13 00:00:00', '2007-02-14 00:00:00']\n",
    "# white = ['2007-03-12 00:00:00', '2007-03-13 00:00:00', '2007-03-14 00:00:00']\n",
    "\n",
    "# v = tr.query(\"tran_date == @valentine\").query(\"gds_grp_nm == @munchies\")\n",
    "# v2 = tr.query(\"tran_date == @valentine\").query(\"gds_grp_mclas_nm == '액세서리'\")\n",
    "\n",
    "\n",
    "# f = pd.merge(v,v2,how='outer')\n",
    "# f['발렌타인'] = 1\n",
    "# f\n",
    "# t = tr\n",
    "# t = t.merge(f,how = 'left').fillna(0)\n",
    "# t = t.groupby('cust_id')['발렌타인'].agg([('발렌타인',np.max)])\n",
    "# features = features.merge(t,on='cust_id', how='left')\n",
    "\n",
    "# w = tr.query(\"tran_date == @white\").query(\"gds_grp_nm == @munchies\")\n",
    "# w2 = tr.query(\"tran_date == @white\").query(\"gds_grp_mclas_nm == '액세서리'\")\n",
    "\n",
    "\n",
    "# f = pd.merge(w,w2,how='outer')\n",
    "# f['화이트'] = 1\n",
    "# f\n",
    "# t = tr\n",
    "# t = t.merge(f,how = 'left').fillna(0)\n",
    "# t = t.groupby('cust_id')['화이트'].agg([('화이트',np.max)])\n",
    "# features = features.merge(t,on='cust_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_cbynm = tr.groupby(['cust_id','gds_grp_mclas_nm'])['amount'].agg([('구매건수', np.size)]).reset_index()\n",
    "tr_cbynm.query(\"구매건수 == 1\")\n",
    "f = tr.groupby('cust_id')['amount'].agg([('구매건수', np.size)])\n",
    "f2 = tr_cbynm.query(\"구매건수 == 1\").groupby('cust_id')['구매건수'].agg([('단독상품군구매수', np.sum)]).reset_index()\n",
    "\n",
    "f = pd.merge(f,f2, on = 'cust_id', how='left').fillna(0)\n",
    "f['단독상품군구매율'] = f['단독상품군구매수']/f['구매건수']\n",
    "f\n",
    "f = f[['cust_id','단독상품군구매율']]\n",
    "features = features.merge(f,on='cust_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['day'] = tr['tran_date'].agg(lambda x: pd.to_datetime(x).day)\n",
    "\n",
    "tr['ee_month'] = tr['day'].agg(lambda x: 'endmonth' if x in list(range(15,31)) else 'earlymonth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = tr.groupby('cust_id')['amount'].agg([('amount', 'sum')]).reset_index()\n",
    "\n",
    "e2 = tr.groupby(['cust_id', 'ee_month'])['amount'].agg([('eamount', 'sum')]).reset_index()\n",
    "\n",
    "e2 = pd.merge(e1, e2, on = 'cust_id', how = 'left')\n",
    "\n",
    "e2['ee_rate'] = e2['eamount'] / e2['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee1 = pd.pivot_table(e2, values = 'ee_rate', index = 'cust_id', columns = 'ee_month',\n",
    "                    aggfunc = sum, fill_value = 0).reset_index()\n",
    "\n",
    "ee1 = ee1.rename_axis(None, axis = 1)\n",
    "\n",
    "f = ee1[['cust_id', 'endmonth']]\n",
    "\n",
    "features = features.merge(f, how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tr.groupby('cust_id')['amount'].agg([('구매액표준편차',np.std)]).reset_index()\n",
    "features = features.merge(f, how='left');features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = ['농산물','가공식품','축산가공','수산품','육류','차/커피','젓갈/반찬','건강식품']\n",
    "daily_life = ['일용잡화','주방용품','식기','생활잡화','침구/수예']\n",
    "clothes = ['디자니어','시티웨어','섬유잡화','캐주얼', \n",
    "           '트래디셔널','기타의류','란제리/내의','모피/피혁', '피혁잡화','남성 트랜디', '남성정장','남성 캐쥬얼']\n",
    "clo = (tr.query('gds_grp_mclas_nm in @ clothes').groupby('cust_id')['amount'].agg('count'))\n",
    "total = (tr.query('gds_grp_mclas_nm in @food or gds_grp_mclas_nm in @ daily_life or gds_grp_mclas_nm in @ clothes')\n",
    "         .groupby('cust_id')['amount'].agg('count'))\n",
    "g = (clo/total).apply(lambda x: 0 if pd.isnull(x) else x).reset_index()\n",
    "f= pd.DataFrame({'cust_id': tr.cust_id.unique()})\n",
    "f = f.merge(g ,how = 'left').amount.apply(lambda x: 0 if pd.isna(x) else x).reset_index()\n",
    "f.columns = ['cust_id','의류_거래율(의류+식품+생활잡화)']\n",
    "features = pd.merge(features, f, how = 'left')\n",
    "display(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_cl = ['디자니어',' 트래디시녈','시티웨어','섬유잡화','명품','캐쥬얼',\n",
    "              '구두','섬유잡화','기타의류','화장품','란제리/내의','모피/피혁','피혁잡화',\n",
    "             '남성 트랜디', '남성정장','남성 캐쥬얼','스포츠', '골프', '셔츠','준보석/시계','액세서리']\n",
    "women = ['화장품','모피/피혁','피혁잡화']\n",
    "man = ['남성 트랜디', '남성정장','남성 캐쥬얼','액세서리','스포츠', '골프','준보석/시계']\n",
    "category_1 = tr.query('gds_grp_mclas_nm in @man').groupby('cust_id')['amount'].agg('count') # 카테코리 남성 제품 거래횟수\n",
    "category_2 = tr[tr.gds_grp_nm.str.contains(\"넥타이|남\")].groupby('cust_id')['amount'].agg('count') # 소분류 중 남성 추정 제품 거래 횟수\n",
    "men_item = (category_1.add(category_2, fill_value =0))\n",
    "fashion = tr.query('gds_grp_mclas_nm in @fashion_cl').groupby('cust_id')['amount'].agg('count') \n",
    "result = ((men_item / fashion).apply(lambda x: 0 if pd.isna(x) else x).\n",
    "          reset_index().rename(columns = {'amount': '패션_대비_남성상품_거래율'}))\n",
    "f_m = pd.DataFrame({'cust_id': tr.cust_id.unique()})\n",
    "f_m = f_m.merge(result, how = 'left')\n",
    "f_m = f_m.패션_대비_남성상품_거래율.apply(lambda x: 0 if pd.isna(x) else x)\n",
    "f_m = f_m.reset_index().rename(columns = {'index': 'cust_id'})\n",
    "features = pd.merge(features, f_m, how = 'left')\n",
    "display(f_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_3 = tr.query('gds_grp_mclas_nm in @women').groupby('cust_id')['amount'].agg('count') \n",
    "category_4 = tr[tr.gds_grp_nm.str.contains(\"여|스타킹|란제리\")].groupby('cust_id')['amount'].agg('count')   \n",
    "women_item = (category_3.add(category_4, fill_value =0))\n",
    "fashion = tr.query('gds_grp_mclas_nm in @fashion_cl').groupby('cust_id')['amount'].agg('count') \n",
    "result = ((women_item / fashion).apply(lambda x: 0 if pd.isna(x) else x).\n",
    "          reset_index().rename(columns = {'amount': '패션_대비_여성상품_거래율'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_w = pd.DataFrame({'cust_id': tr.cust_id.unique()})\n",
    "f_w = f_w.merge(result, how = 'left')\n",
    "f_w = f_w.패션_대비_여성상품_거래율.apply(lambda x: 0 if pd.isna(x) else x)\n",
    "f_w = f_w.reset_index().rename(columns = {'index': 'cust_id'})\n",
    "features = pd.merge(features, f_w, how = 'left')\n",
    "display(f_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_item = ((tr[tr.amount<0].groupby('cust_id')['tran_date'].agg('nunique') /\n",
    "            tr.groupby('cust_id')['tran_date'].agg('nunique')).apply(lambda x: 0 if pd.isna(x) else x))\n",
    "display(re_item)\n",
    "rt_mean = re_item.mean()\n",
    "rt_std = re_item.std()\n",
    "f = (((re_item)-(rt_mean))/rt_std).reset_index().rename(columns = {'tran_date' : '전체방문_중_환불방문(표준화)'})\n",
    "features = pd.merge(features, f, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['tran_date'] = tr.tran_date.astype('datetime64')\n",
    "tr['date'] = tr.tran_date.apply(lambda x : x.weekday())\n",
    "tr['month'] = tr.tran_date.apply(lambda x : x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = pd.pivot_table(tr, columns = 'month',values = 'tran_date', index = 'cust_id', aggfunc= 'nunique',fill_value = 0);\n",
    "a1 = a1.reset_index()\n",
    "del a1['cust_id']\n",
    "def point(x):\n",
    "    a= 1\n",
    "    re_go = 0\n",
    "    for i in range(11):\n",
    "        if x[a]>=1 and x[a+1]>=1:\n",
    "            re_go = re_go+1\n",
    "            a= a+1\n",
    "        else: \n",
    "            a= a+1\n",
    "    return re_go\n",
    "result = a1.apply(lambda x: point(x), axis =1)\n",
    "result = (result / 11).reset_index()\n",
    "result.columns = ['cust_id', '월_재방문율']\n",
    "features = pd.merge(features, result, how = 'left')\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season(x):\n",
    "    m = x.month\n",
    "    d = x.day\n",
    "    if 21<= d <= 31 and 3<= m <= 5:\n",
    "        return('봄 시즌 마감')\n",
    "    elif 21<= d <= 31 and 6<= m <=8:\n",
    "        return('여름 시즌 마감')\n",
    "    elif 21<= d <= 31 and 9<= m <=11:\n",
    "        return('가을 시즌 마감')\n",
    "    elif 21<= d <= 31 and (12 == m or m <=2):\n",
    "        return('겨울 시즌 마감')\n",
    "    else :\n",
    "        return('그 외')    \n",
    "\n",
    "tr['season_sale'] = pd.to_datetime(tr.tran_date).apply(season)\n",
    "f = pd.pivot_table(tr, index='cust_id', columns='season_sale', values='amount', \n",
    "                   aggfunc=np.size, fill_value=0).reset_index()\n",
    "af = pd.DataFrame(f['봄 시즌 마감']+f['여름 시즌 마감']+f['가을 시즌 마감']+f['겨울 시즌 마감']).reset_index()\n",
    "af = af.rename({'index':'cust_id',0:'시즌 마감'},axis=1)\n",
    "ss = pd.DataFrame(af['시즌 마감']/features['내점일수']).reset_index()\n",
    "ss = ss.rename({'index':'cust_id',0:'시즌 마감 방문비율'},axis=1)\n",
    "features = features.merge(ss,on='cust_id', how='left'); features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tr.groupby(['cust_id','store_nm'])['amount'].agg('count').reset_index()\n",
    "t\n",
    "f = t.groupby('cust_id')['amount'].agg([('주구매지점재방문률',lambda x :((x.max()-1) / x.sum()) * 100)]).reset_index()\n",
    "f.주구매지점재방문률 = f.주구매지점재방문률.round(2)\n",
    "features = features.merge(f, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =tr.groupby(['cust_id','tran_date'])['amount'].sum().reset_index().groupby('cust_id')['amount'].agg(\n",
    "                                                    [('내점시마다 구매액의 일관성',lambda x : np.var(x))]).reset_index();f\n",
    "features = features.merge(f, how='left'); features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features1 = features.copy(deep=True)\n",
    "features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = list(features1.select_dtypes(include=['object']).columns)\n",
    "num_features = [c for c in features1.iloc[:,1:].columns.tolist() if c not in cat_features]\n",
    "\n",
    "features1[num_features] = SimpleImputer(strategy='constant', fill_value=0).fit_transform(features1[num_features])\n",
    "features1[cat_features] = SimpleImputer(strategy=\"most_frequent\").fit_transform(features1[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(features1[cat_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1[num_features] = features1[num_features].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features1.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_level = 'gds_grp_nm'\n",
    "\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=False))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('cust_id')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('cust_id')[p_level].agg(oversample, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the Word2Vec model\n",
    "num_features1 = 100 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        size=num_features1, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "pd.concat([pd.DataFrame({'cust_id': np.sort(train['cust_id'].unique())}), train_features], axis=1).to_csv('X_train.csv', index=False)\n",
    "pd.concat([pd.DataFrame({'cust_id': np.sort(test['cust_id'].unique())}), test_features], axis=1).to_csv('X_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(['python', 'word2vec.py'], env={**os.environ, 'PYTHONHASHSEED': '123'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stand = features1.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_stand = list(features_stand.select_dtypes(include=['object']).columns)\n",
    "num_features_stand = [c for c in features_stand.iloc[:,1:].columns.tolist() if c not in cat_features_stand]\n",
    "\n",
    "\n",
    "features_stand[cat_features_stand] = features_stand[cat_features_stand].fillna('None')\n",
    "features_stand[num_features_stand] = features_stand[num_features_stand].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features_stand[num_features_stand]= scaler.fit_transform(features_stand[num_features_stand])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stand = pd.get_dummies(features_stand)\n",
    "features_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarized data and word2vec data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stand = pd.concat([pd.concat([train_id, test_id]).reset_index(drop=True), features_stand], axis=1)\n",
    "X_train_stand = features_stand.query('cust_id in @train_id').drop('cust_id', axis=1)\n",
    "X_test_stand = features_stand.query('cust_id in @test_id').drop('cust_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = pd.concat([X_train_stand, train_features], axis=1)\n",
    "X_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_stand = X_test_stand.reset_index(drop = True)\n",
    "X_test_cat = pd.concat([X_test_stand, test_features],axis=1)\n",
    "X_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 DataFrame 불러오기\n",
    "X_train_cat = pd.read_csv(os.path.abspath(\"../dat\")+'/X_train_cat_stand.csv')\n",
    "X_test_cat = pd.read_csv(os.path.abspath(\"../dat\")+'/X_test_cat_stand.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "gbm = GradientBoostingClassifier(learning_rate = 0.1, max_depth = 8, max_features = 'sqrt', random_state=0)\n",
    "\n",
    "models = [gbm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Stratified cross Validation\"을 통해 우리는 더 나은 점수를 받는다.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "stra = StratifiedKFold(n_splits = 10, random_state = 0)\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "# k_fold = KFold(n_splits = 5, random_state = 0)\n",
    "\n",
    "# from sklearn.model_selection import ShuffleSplit\n",
    "# shuffle_split = ShuffleSplit(test_size = .5, train_size = 0.5, n_splits = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    cv_scores = []\n",
    "    \n",
    "    for p in tqdm(range(5,100,1)):\n",
    "        X_new = SelectPercentile(percentile=p).fit_transform(X_train_cat, y_train)    \n",
    "        cv_score = cross_val_score(model, X_new, y_train, scoring='roc_auc', cv=stra).mean()\n",
    "        cv_scores.append((p,cv_score))\n",
    "\n",
    "    best_score = cv_scores[np.argmax([score for _, score in cv_scores])]\n",
    "    print(model.__class__.__name__, best_score)\n",
    "\n",
    "    plt.plot([k for k, _ in cv_scores], [score for _, score in cv_scores])\n",
    "    plt.xlabel('Percent of features')\n",
    "    plt.legend(models)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_range = [9, 21, 32, 34, 43, 68]\n",
    "\n",
    "# for model in models:\n",
    "    \n",
    "#     cv_scores_mean = []\n",
    "    \n",
    "#     for percentile in tqdm(new_range):\n",
    "    \n",
    "#         X_new = SelectPercentile(percentile = percentile).fit_transform(X_train_cat, y_train)\n",
    "#         cv_score = cross_val_score(model, X_new, y_train, scoring='roc_auc', cv=stra)\n",
    "#         cv_scores_mean.append((percentile, cv_score.mean()))\n",
    "#         print(cv_score, cv_score.mean())\n",
    "        \n",
    "\n",
    "#     best_score = cv_scores_mean[np.argmax([score for _, score in cv_scores_mean])]\n",
    "#     print(model.__class__.__name__, best_score)\n",
    "    \n",
    "\n",
    "#     plt.plot([p for p,_ in cv_scores_mean], [score for _, score in cv_scores_mean])\n",
    "#     plt.xlabel('Percent of features')\n",
    "#     plt.legend(loc=0)\n",
    "#     plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = SelectPercentile(percentile=best_score[0]).fit(X_train_cat, y_train)\n",
    "X_train_cat = fs.transform(X_train_cat)\n",
    "X_test_cat = fs.transform(X_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_p = SelectPercentile(percentile = 32).fit(X_train_cat, y_train)\n",
    "# X_train_cat1 = select_p.transform(X_train_cat)\n",
    "# X_test_cat1 = select_p.transform(X_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat, X_dev, y_train, y_dev = train_test_split(X_train_cat, y_train, test_size=0.3,  stratify= y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### models assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "#     (\n",
    "#         KNeighborsClassifier(),\n",
    "#         {'n_neighbors': range(1,100),        \n",
    "#          'weights': ['distance'],\n",
    "#          'metric': ['manhattan']}\n",
    "#     ),\n",
    "    (\n",
    "        LogisticRegression(random_state=0),  \n",
    "        {'C': [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.001],      \n",
    "         'penalty': ['l2']}\n",
    "    ),\n",
    "    (\n",
    "        RandomForestClassifier(random_state=0),\n",
    "        {'n_estimators': range(200, 300),\n",
    "         'max_depth': range(3,24),\n",
    "         'min_impurity_decrease': np.arange(0.001,0.004, 0.001),\n",
    "         'max_features': ['sqrt'],\n",
    "         'bootstrap': [False],\n",
    "         'criterion': ['entropy'], \n",
    "         'class_weight': [{}],\n",
    "         'min_samples_split': range(1,20,1),\n",
    "         'min_samples_leaf': range(4,20,1)}\n",
    "    ),\n",
    "#      (\n",
    "#          XGBClassifier(random_state=0),\n",
    "#          {'n_estimators': range(10, 310, 10),\n",
    "#           'learning_rate': np.arange(0.0, 0.501, 0.001),\n",
    "#           'subsample': [0.2, 0.3, 0.5, 0.7, 0.9, 1],\n",
    "#           'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "#           'colsample_bytree': [0.5, 0.7, 0.9, 1],\n",
    "#           'min_child_weight': [1, 2, 3, 4],\n",
    "#           'reg_alpha': [1e-07,1e-06,0.0001,0.001,0.01,0.0005,0.005,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.7,1,2,3,4,5,10],\n",
    "#           'reg_lambda': [1e-07,1e-06,0.0001,0.001,0.01,0.0005,0.005,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.7,1,2,3,4,5,10],\n",
    "#           'scale_pos_weight': np.arange(0.0, 50.1, 0.1)}\n",
    "#    ),\n",
    "#     (\n",
    "#         LGBMClassifier(random_state=0),\n",
    "#         {'n_estimators': range(10, 310, 10),\n",
    "#          'learning_rate': np.arange(0.0, 0.501, 0.001),\n",
    "#          'num_leaves': [10,20,30,40,50,60,70,80,90,100,150,200],\n",
    "#          'min_split_gain': [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "#          'reg_alpha': [1e-07,1e-06,0.0001,0.001,0.01,0.0005,0.005,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.7,1,2,3,4,5,10],\n",
    "#          'reg_lambda': [1e-07,1e-06,0.0001,0.001,0.01,0.0005,0.005,0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.7,1,2,3,4,5,10],\n",
    "#          'feature_fraction': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#          'bagging_fraction': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#          'bagging_freq': [1, 2, 3, 4, 5, 6, 7],\n",
    "#          'min_child_samples': range(5,105,5)}\n",
    "#      ),\n",
    "#       (\n",
    "#          ExtraTreesClassifier(random_state=0),\n",
    "#          {'n_estimators': (300, 700),\n",
    "#           'max_depth': [50, 75, 90, 100, 200, 300],\n",
    "#           'max_features' : range(40,80,1),\n",
    "#           'criterion' : ['entropy']}\n",
    "#       ),\n",
    "#      (\n",
    "#         CatBoostClassifier(random_state=0, verbose=False),\n",
    "#         {'n_estimators': range(10, 310, 10),\n",
    "#          'depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "#          'random_strength': [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],\n",
    "#          'l2_leaf_reg': [1,2,3,4,5,6,7,8,9,10,20,30,50,100,200]}\n",
    "#     ),\n",
    "    (\n",
    "        GradientBoostingClassifier(random_state=0),\n",
    "        {'n_estimators': range(130, 200),\n",
    "         'learning_rate': np.arange(0.0, 0.3, 0.001),\n",
    "         'min_samples_split': range(3, 7, 1),\n",
    "         'min_samples_leaf': [1, 2],\n",
    "         'max_depth': [1, 2],\n",
    "         'min_impurity_decrease': np.arange(0.005,0.035, 0.005),\n",
    "         'max_features': ['auto']}\n",
    "    ),\n",
    "#     (\n",
    "#         AdaBoostClassifier(random_state=0),\n",
    "#         {'n_estimators': range(150, 250),\n",
    "#          'learning_rate': np.arange(0.005, 0.2, 0.001),}\n",
    "#     ),\n",
    "    (\n",
    "        MLPClassifier(random_state=0),\n",
    "        {'batch_size': [64, 128, 256, 512, 1024],\n",
    "         'learning_rate' : ['constant'],\n",
    "         'activation': ['logistic'],\n",
    "         'solver': ['sgd'],\n",
    "         'alpha': np.arange(0.01, 0.5, 0.01),\n",
    "         'hidden_layer_sizes': [(256,),(32,),(64,),(128,),(512,)]}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clfs_tuned = []  \n",
    "\n",
    "for clf, param_grid in tqdm(clfs):\n",
    "    start = time.time()\n",
    "    clf_name = type(clf).__name__\n",
    "    iterations = 150 if clf_name in ['MLPClassifier'] else 250\n",
    "    rand_search = RandomizedSearchCV(clf, param_grid, n_iter=iterations, scoring='roc_auc', \n",
    "                                     cv=stra, random_state=0, n_jobs=-1)\n",
    "    rand_search.fit(X_train_cat, y_train)\n",
    "    clf_score = rand_search.score(X_dev, y_dev)\n",
    "    print((clf_name, clf_score))\n",
    "    clfs_tuned.append((clf_name, rand_search, clf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clfs_tuned)):\n",
    "    print('{}: {}\\n'.format(clfs_tuned[i][0] , clfs_tuned[i][1].best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. **Model Ensemble**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = []\n",
    "for name, clf, clf_score in clfs_tuned:\n",
    "    pred = clf.predict_proba(X_dev)[:,1]\n",
    "    name = f'{name} \\n({clf_score:.4f})'\n",
    "    pred_results.append(pd.Series(pred, name=name))\n",
    "ensemble_results = pd.concat(pred_results, axis=1)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "g = sns.heatmap(ensemble_results.corr(), annot=True, cmap='Blues')\n",
    "g.set_title(\"Correlation between models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ensemble_results.corr().sum()-1)/(ensemble_results.corr().shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = (ensemble_results.corr().sum()-1)/(ensemble_results.corr().shape[0]-1)\n",
    "names = corr.index\n",
    "aucs = np.array(corr.index.str[-7:-1]).astype(float)\n",
    "df = pd.DataFrame({'model': names, 'auc': aucs, 'cor': corr})        \n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "g = sns.scatterplot(x=\"cor\", y=\"auc\", data=df, s=40, color='red')\n",
    "for line in range(0, df.shape[0]):\n",
    "     g.text(df.cor[line]+0.003, df.auc[line]-0.003, \n",
    "            df.model[line], horizontalalignment='left', \n",
    "            size='medium', color='black', weight='semibold')\n",
    "        \n",
    "plt.xlim((df.cor.min()-0.01,df.cor.max()+0.01))\n",
    "plt.ylim((df.auc.min()-0.01,df.auc.max()+0.01))\n",
    "plt.xlabel('Mean Agreement')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [\n",
    "#     'KNeighborsClassifier', \n",
    "    'MLPClassifier',\n",
    "    'LogisticRegression', \n",
    "    'RandomForestClassifier', \n",
    "    'GradientBoostingClassifier',\n",
    "#     'AdaBoostClassifier',\n",
    "#     'CatboostClassifier',\n",
    "#     'XGBClassifier',\n",
    "#     'LGBMClassifier'\n",
    "]\n",
    "\n",
    "models_for_ensemble = [clf for clf in clfs_tuned if clf[0] in selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "for p in tqdm([0, 1, 2.56]):  \n",
    "    for i in range(2, len(models_for_ensemble)+1):\n",
    "        for models in combinations(models_for_ensemble, i):\n",
    "            if p == 0:\n",
    "                pred_mean = gmean([clf.predict_proba(X_dev)[:,1] for name, clf, _ in models], axis=0)\n",
    "            else:\n",
    "                preds = [clf.predict_proba(X_dev)[:,1] for name, clf, _ in models]\n",
    "                pred_mean = (np.sum(np.array(preds)**p, axis=0) / len(models))**(1/p)\n",
    "            score = roc_auc_score(y_dev, pred_mean)\n",
    "            if max_score < score:\n",
    "                best_avg_ensemble = (p, models, score)\n",
    "                max_score = score\n",
    "\n",
    "p, models, score = best_avg_ensemble\n",
    "print('p={}\\n{}\\n{}'.format(p, '●'.join([clf_name for clf_name, _, _ in models]), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingClassifier(ClassifierMixin):\n",
    "    def __init__(self, estimators, p):\n",
    "        self.estimators = estimators\n",
    "        self.p = p\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return None\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.p == 0:\n",
    "            pred = gmean([clf.predict(X) for name, clf in estimators], axis=0)\n",
    "        else:\n",
    "            preds = [clf.predict(X) for name, clf in estimators]\n",
    "            pred = (np.sum(np.array(preds)**self.p, axis=0) / len(self.estimators))**(1/self.p)\n",
    "        return pred\n",
    "         \n",
    "    def predict_proba(self, X):\n",
    "        if self.p == 0:\n",
    "            prob = gmean([clf.predict_proba(X) for name, clf in estimators], axis=0)\n",
    "        else:\n",
    "            probs = [clf.predict_proba(X) for name, clf in estimators]\n",
    "            prob = (np.sum(np.array(probs)**self.p, axis=0) / len(self.estimators))**(1/self.p)\n",
    "        return prob\n",
    "    \n",
    "estimators = [(name, clf) for name, clf, _ in models]\n",
    "avg_clf = AveragingClassifier(estimators, p)\n",
    "avg_clf.fit(X_train_cat, y_train)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2-layer stacking\n",
    "# selected1 = [\n",
    "# #     'KNeighborsClassifier', \n",
    "#     'MLPClassifier',\n",
    "#     'LogisticRegression', \n",
    "#     'RandomForestClassifier', \n",
    "# #     'GradientBoostingClassifier',\n",
    "# #     'AdaBoostClassifier',\n",
    "# #     'CatboostClassifier',\n",
    "# #     'XGBClassifier',\n",
    "# #     'LGBMClassifier'\n",
    "# ]\n",
    "\n",
    "# estimators = [(name, clf) for name, clf, _ in clfs_tuned if name in selected1]\n",
    "# stk_clf = StackingClassifier(\n",
    "#     estimators=estimators, final_estimator = LogisticRegression(random_state=0), cv=5)\n",
    "\n",
    "# stk_clf.fit(X_train_cat, y_train)\n",
    "# print(roc_auc_score(y_dev, stk_clf.predict_proba(X_dev)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. **Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging ensemble을 적용한 submission: (결과)\n",
    "pd.DataFrame({'cust_id':np.arange(3500, 5982), 'gender': avg_clf.predict_proba(X_test_cat)[:,1]}).to_csv('송한솔_02.csv', index=False)\n",
    "# Stacking ensemble을 적용한 submission: (결과)\n",
    "# pd.DataFrame({'cust_id':np.arange(3500, 5982), 'gender': stk_clf.predict_proba(X_test_cat)[:,1]}).to_csv('송한솔-11-26-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.1 on Python 3.7 (CUDA 10.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
